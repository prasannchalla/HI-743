---
title: "Logistic Regression and Classification"
author: "Lakshmi Prasanna Challa"
date: "2025-02-27"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nnet)
library(ISLR2)
```

# 1. INTRODUCTION

Logistic Regression (binary), Multiple Logistic Regression, and Multinomial Logistic Regression.These models will be used to predict categorical outputs like credit default or sales categories.

# 2. Data

Load and inspect the dataset structure. We are using here the 'Default' dataset to understand credit default behavior.

```{r}
data = Default
str(data)
```

## 2.1 Visualizing the data

### 2.1.1 Distribution of Balance

This histogram shows the distribution of credit card balance and its relation to default status. We expect balances on the higher side to have a greater likelihood of default.

```{r balance distribution}
ggplot(data, aes(x =balance, fill=default)) +
  geom_histogram(bins = 30, alpha = 0.7, position = 'identity' ) +
  labs(title = "Distribution of balance by Default Status",
       x = "Balance",
       y = "Count?")
 
```

### 2.1.2 Distribution of Income

This plot shows income distribution based on default status. It helps establish whether income has any role in predicting default.

```{r}
ggplot(data, aes(x = income, fill = default)) +
  geom_histogram(bins = 30, alpha = 0.7, position = 'identity') +
  labs(title = "distribution of Income by Default Status",
       x = "Income",
       y = "Count")
```

```{r}
ggplot(data, aes(x = income, fill = student)) +
  geom_histogram(bins = 30, alpha = 0.7, position = 'identity') +
  labs(title = "distribution of Income by Default Status",
       x = "Income",
       y = "Count")
```

### 2.1.3 Student Status by Default

A bar chart that contrasts the number of defaulting students and non-students. It can be utilized to identify if student status is associated with higher risk of default.

```{r}
ggplot(data, aes(x = student, fill = default)) +
  geom_bar(position ='dodge') +
  labs(title = "distribution of Income by Student Status",
       x = "Student",
       y = "Count")
```

# 4. Logistic Regression

### 4.1 Fitting the Model

Logistic regression is a classification algorithm that can be utilized to forecast the probability of a binary event. Here, we are using it for default status prediction solely on the basis of the balance factor.

```{r}
logit_model = glm(default ~ balance, data = data, family = binomial)
summary(logit_model)
```

```{r}
data$predicted_prob = predict(logit_model, type = "response")
head(data)
```

### 4.2 Evaluate Model Performance

Converts the probabilities to binary predictions by applying a threshold value (defaulted as 0.5) and evaluates the model on confusion matrix and overall accuracy.

```{r}
threshold = 0.5
data$predicted_default = ifelse(data$predicted_prob > threshold, "yes", "No")
conf_matrix = table(data$predicted_default, data$default)
conf_matrix
```

```{r}
accuracy = sum(diag(conf_matrix)) / sum(conf_matrix)
accuracy
```

# 5. Multiple Logistic Regression

## 5.1 Fitting the Model

Here, we will include an **interaction term** between 'income' and 'student' that allows the effect of 'income'

```{r}
logit_mult_model = glm(default ~ balance + income * student, data=data, 
family=binomial)
summary(logit_mult_model)
```

## 5.2 Evaluating the Model

Use the trained multiple logistic regression model to make default predictions and evaluate the model based on confusion matrix and accuracy.

```{r}
data$mult_predicted_prob = predict(logit_mult_model, type = "response")
data$mult_predicted_default = ifelse(data$mult_predicted_prob > threshold, "Yes", "No")
conf_matrix_mult = table(data$mult_predicted_default, data$default)
conf_matrix_mult
```

```{r}
accuracy_mult = sum(diag(conf_matrix_mult)) / sum(conf_matrix_mult)
accuracy_mult
```

# 6. Multinomial Logistic Regression

## 6.1 Load the Data

Import Carseats dataset and create a new categorical variable 'SalesCategory' by categorizing Sales as Low, Medium, High classes.

```{r}
data2 = Carseats
data2$SalesCategory = cut(data2$Sales, breaks = 3, labels = c("Low", "Medium", "High"))
```

```{r}
multi_model = multinom(SalesCategory ~ Price + Income +Advertising,
data = data2)
summary(multi_model)
```

## 6.2 Make Predictions

Make predictions of the sales category for each record using the multinomial model. 

```{r}
data2$nomial_predicted_salesCat = predict(multi_model)
head(data2)
```

## 6.3 Evaluate Model

Evaluate the multinomial model's performance by analyzing predicted vs actual categories and overall prediction accuracy.

```{r}
conf_matrix_multi = table(data2$nomial_predicted_salesCat, data2$SalesCategory)
conf_matrix_multi
```

```{r}
accuracy_multi = sum(diag(conf_matrix_multi)) / sum(conf_matrix_multi) 
accuracy_multi
```

# Assignment Section

# Background

Diabetes is a chronic disease affecting millions of individuals worldwide. Early detection through predictive modelling can help guide prevention and treatment. in this assignment , you will use logistic regression to predict whether an individual has diabetes using basic health information.

We will use the Pima Indians Diabetes Dataset, a commonly used dataset in health informatics available from the UCI Machine Learning Repository and built into the mlbench R package.

# Simple Logistic Regression

We use logistic regression to predict diabetes presence based on glucose levels. This binary classification model estimates the probability of a person having diabetes, where the glucose level serves as the sole predictor variable. The output is a probability score, with a threshold determining the prediction of either diabetes (1) or no diabetes (0).

```{r}
install.packages("mlbench")
library(mlbench)
data(PimaIndiansDiabetes)
df = PimaIndiansDiabetes
```

## Data Exploration and Summary Figures

To explore the data, we create a histogram that visualizes the distribution of glucose levels for individuals, split by diabetes status (positive or negative). This allows us to compare the glucose levels between diabetic and non-diabetic groups, helping identify if there are significant differences in glucose levels. A clear separation or overlap in the distributions can indicate patterns useful for building a predictive model.

```{r}
ggplot(PimaIndiansDiabetes, aes(x = glucose, fill = diabetes)) +
  geom_histogram(bins = 30, alpha = 0.7, position = 'identity') +
  labs(title = "Distribution of Glucose by Diabetes Status",
       x = "Glucose",
       y = "Count")
```

## Fit a Simple Logistic Regression Model (Train & Test Split)

We split the Pima Indians Diabetes dataset into 70% training and 30% testing. A logistic regression model is fitted using glucose as the sole predictor with the `glm()` function (binomial family). The model is trained on the training set, and the `summary()` function provides the coefficients and fit details.

```{r}
set.seed(123)
split = sample(1:nrow(PimaIndiansDiabetes), 0.7 * nrow(PimaIndiansDiabetes))
train = PimaIndiansDiabetes[split, ]
test = PimaIndiansDiabetes[-split, ]

simple_model = glm(diabetes ~ glucose, data = train, family = "binomial")
summary(simple_model)
```

## Interpret Coefficients & Apply the Model for Prediction on the test Data

We apply the trained logistic regression model to the test set using the `predict()` function to generate predicted probabilities. These probabilities are converted into binary outcomes ("pos" for positive diabetes, "neg" for negative diabetes) with a 0.5 threshold. We then use the `table()` function to create a confusion matrix, comparing the predicted labels with the actual test labels, allowing us to assess the model's performance on new data.

```{r}
preds_test = ifelse(predict(simple_model, newdata = test, type = "response") > 0.5, "pos", "neg")
table(Predicted = preds_test, Actual = test$diabetes)
```

# Multiple Logistic Regression

## Fit a Multiple Logistic Regression \`model (Train & Test Split)

We enhance the logistic regression model by incorporating multiple predictors—glucose, BMI, age, and blood pressure—to predict diabetes likelihood. The `glm()` function with a binomial family is applied to the training data, and the `summary()` function provides coefficients, showing the impact of each variable on diabetes probability.

```{r}
multi_model = glm(diabetes ~ glucose + mass + age + pressure, data = train, family = "binomial")
summary(multi_model)
```

## Interpret Coefficients &Apply the Model for Prediction on Test Data

The trained multiple logistic regression model is applied to the test data with the `predict()` function to generate predicted probabilities. These are then classified into binary outcomes ("pos" or "neg") using a 0.5 threshold with `ifelse()`. A confusion matrix is created to compare predicted and actual diabetes statuses, enabling performance evaluation on the test set.

```{r}
preds_test = ifelse(predict(simple_model, newdata = test, type = "response") > 0.5, "pos", "neg")
table(Predicted = preds_test, Actual = test$diabetes)
```

# K-Nearest Neighbors Classification

K-Nearest Neighbors (KNN) is a simple, flexible algorithm that makes predictions based on the majority class of the closes data points.

Use the `caret` and `class` libraries with the `knn()` function. see our in-class lab for a worked example.

## Prepare the Data

```{r}
install.packages("caret")
library(caret)
library(class)

train_knn = train
test_knn = test

train_X = scale(train_knn[, c("glucose", "mass", "age", "pressure")])
test_X = scale(test_knn[, c("glucose", "mass", "age", "pressure")])

train_y = train_knn$diabetes
test_y = test_knn$diabetes
```

## Fit a KNN Classifies Model (Train & Test Split)

The K-Nearest Neighbors (KNN) algorithm is applied to classify diabetes status using the `knn()` function, with `k = 5` and training data (train_X, train_y). The `table()` function creates a confusion matrix to compare predicted (knn_preds) and actual (test_y) diabetes statuses, allowing evaluation of the model's accuracy.

```{r}
set.seed(123)
knn_preds = knn(train = train_X, test = test_X, cl = train_y, k = 5)
table(Predicted = knn_preds, Actual = test_y)
```

## Interpret & Apply to Test Data

After fitting the K-Nearest Neighbors (KNN) model to the training data and predicting on the test set, Compute the following performance metrics:

-   **Accuracy**: (True Positives + True Negatives) / Total = (124 + 49) / 231 = 0.749 (or 74.9%)

-   **Precision (positive class)**: True Positives / (True Positives + False Positives) = 49 / (49 + 26) ≈ 0.653 (or 65.3%)

-   **Recall (positive class)**: True Positives / (True Positives + False Negatives) = 49 / (49 + 32) ≈ 0.605 (or 60.5%)

**Interpretation:**

The KNN model achieved 74.9% accuracy, correctly predicting most cases. With a precision of 65.3%, it was accurate 65% of the time when diagnosing diabetes. Its recall of 60.5% means it identified 60% of actual diabetes cases. While performance is solid, the relatively low recall could be problematic in medical settings, where missing a diagnosis is critical.

# Model Comparison and Discussion

In this project, we implemented and evaluated three classification models to predict diabetes:

| Model | Key Features | Accuracy | Strengths |
|----|----|----|----|
| Simple Logistic Regression | Uses only glucose as a predictor | \~72-74% | Easy to interpret; simple model |
| Multiple Logistic Regression | Uses glucose, BMI, age, and blood pressure | 76-78% | Higher accuracy; considers multiple risk Factors |
| K-Nearest Neighbors (KNN) | Non-parametric; sensitive to scaling and choice of k | 74.9% | Flexible model; performs similarly to logistic regression |

**Discussion**:

-   **Accuracy**: Multiple logistic regression achieved the highest accuracy, benefiting from additional predictors. KNN and simple logistic regression performed similarly, but adding more features in multiple logistic regression improved predictions.

-   **Strengths & Limitations**: Simple logistic regression is the most interpretable, while KNN offers flexibility but needs careful tuning (scaling and choice of k). Multiple logistic regression strikes a balance between interpretability and predictive power by considering more features.

## Conclusion:

Multiple logistic regression is the preferred model, offering a strong balance between accuracy and interpretability. Its use of multiple predictors provides a more comprehensive risk assessment while remaining transparent—an important factor in healthcare decision-making. While KNN performs comparably and can be a good alternative with proper tuning, its sensitivity to parameters and lower interpretability make logistic regression the more practical choice in clinical settings.
